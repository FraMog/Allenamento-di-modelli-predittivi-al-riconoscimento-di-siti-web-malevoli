{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mi sembra più leggibile mettere questa riga in una casella diversa dagli import\n",
    "data = pd.read_csv(\"Matriz_Completa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424 train +  356 test\n"
     ]
    }
   ],
   "source": [
    "# NON abbiamo più bisogno di questa funzione, il suo compito è rimpiazzato da quella nella casella successiva\n",
    "'''\n",
    "def split_train_test(d, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(d))\n",
    "    test_set_size = int(len(d)*test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return d.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(data, 0.2)\n",
    "print(len(train_set), \"train + \", len(test_set), \"test\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La classe StratifiedShuffleSplit esegue uno split del dataset in train_set e test_set rispettando la\n",
    "# distribuzione dei valori di tutto il dataset. In questo caso stiamo stratificando rispetto alla colonna TIPO.\n",
    "# L'attributo test_size = 0.2 indica che il test_set comprende il 20% dei valori dell'intero dataset\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "for train_index, test_index in split.split(data, data[\"TIPO\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benigna    0.878652\n",
       "Maligna    0.121348\n",
       "Name: TIPO, dtype: float64"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui abbiamo le percentuali di siti Benigni e Maligni nel dataset\n",
    "\n",
    "data[\"TIPO\"].value_counts()/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424 356\n"
     ]
    }
   ],
   "source": [
    "# Qui abbiamo le dimensioni di test_set e train_set\n",
    "\n",
    "print(len(strat_train_set), len(strat_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Benigna    0.878511\n",
       "Maligna    0.121489\n",
       "Name: TIPO, dtype: float64"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Come possiamo vedere, le percentuali sono state rispettate\n",
    "\n",
    "strat_train_set[\"TIPO\"].value_counts()/len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValidIndex(invalidIndex, lenght):\n",
    "    indici = np.array(range(lenght))\n",
    "    valid = np.delete(indici, invalidIndex)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Column_selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col_indexes):\n",
    "        self.col_indexes = col_indexes\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.col_indexes].reshape(len(X[:, self.col_indexes]), len(self.col_indexes))\n",
    "    \n",
    "class StringUpper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        r = [str(element).upper() for element in X]\n",
    "        return np.array(r).reshape(-1, 1)\n",
    "    \n",
    "class ColumnLabelBinarizer(LabelBinarizer):\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return super(ColumnLabelBinarizer,self).fit_transform(X)\n",
    "    \n",
    "class NumberCheckNan(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        for i in range (0, len(X)):\n",
    "            for j in range(0, len(X[0])):\n",
    "                if X[i][j] is None or X[i][j]!= X[i][j] or X[i][j] == \"None\":\n",
    "                    X[i][j] = 0\n",
    "        return np.array(X).reshape(-1, len(X[0]))\n",
    "    \n",
    "    \n",
    "class CheckNan(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        for i in range (0, len(X)):\n",
    "            if X[i]!= X[i]:\n",
    "                X[i] = \"None\"\n",
    "        return np.array(X).reshape(-1, 1)\n",
    "    \n",
    "class DataFrameToArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None  \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):        \n",
    "        return X.values\n",
    "\n",
    "    \n",
    "class ApacheTransform(BaseEstimator, TransformerMixin):\n",
    "    #col_indexes Indici sui quali la pipeline deve operare\n",
    "    def __init__(self):\n",
    "        return None  \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        #print(X)\n",
    "        #X è t1 passato come parametro in c= Server_Pipeline.fit_transform(t1)\n",
    "        Y=np.array([])\n",
    "        for i in range(0, len(X)):\n",
    "            Y= np.append(Y,int(\"APACHE\" in X[i]))\n",
    "        return Y.reshape(-1, 1)\n",
    "\n",
    "class CacheHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.value_list = {}\n",
    "        return None\n",
    "    def fit(self, X, y= None):\n",
    "        X = X.tolist()\n",
    "        for j in range (0, len(X)):\n",
    "            value = X[j]\n",
    "            if X[j] is None:\n",
    "                cell_values=\"None\"\n",
    "            else:\n",
    "                cell_values= \"\".join(X[j][0].split(\" \")).split(\";\")\n",
    "            for cell_value in cell_values:\n",
    "                if cell_value not in self.value_list:\n",
    "                    self.value_list[cell_value] = len(self.value_list.keys())\n",
    "        return  self  \n",
    "    def transform(self, X):\n",
    "        matrix=[]\n",
    "        for i in range (0, len(X)):\n",
    "            cell_values=\"\"\n",
    "            vettoreIesimaCella= (np.zeros(len(self.value_list.keys()),dtype=int)).tolist()\n",
    "            if X[i] is None:\n",
    "                cell_values=\"None\"\n",
    "            else:\n",
    "                cell_values= \"\".join(X[i][0].split(\" \")).split(\";\")\n",
    "            for cell_value in cell_values:\n",
    "                if cell_value in self.value_list:\n",
    "                    indexToChange=self.value_list[cell_value]\n",
    "                    vettoreIesimaCella[indexToChange]=1 \n",
    "            matrix.append(vettoreIesimaCella) \n",
    "        return np.array(matrix).reshape(-1, len(matrix[0]))\n",
    "    \n",
    "class extension_extractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        result = []\n",
    "        for i in range(len(X)):\n",
    "            substrings = X[i][0].split(\".\") \n",
    "            result.append(substrings[len(substrings)-1])\n",
    "        return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prePipeline = Pipeline([\n",
    "    ('dataFrameSelector', DataFrameToArray()),\n",
    "    ('column_selector', Column_selector(col_indexes=getValidIndex([0,9,10], len(strat_train_set.columns))))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "charsetPipeline = Pipeline([\n",
    "    ('Column_selector', Column_selector(col_indexes=[2])),\n",
    "    ('checkNan', CheckNan()),\n",
    "    ('stringUpper', StringUpper()),\n",
    "    ('columnLabelBinarizer',ColumnLabelBinarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Server_Apache_Pipeline = Pipeline([\n",
    "     (\"column_selector\", Column_selector(col_indexes=[3])),\n",
    "     (\"CheckNan\", CheckNan()),\n",
    "     (\"StringUpper\", StringUpper()),\n",
    "     (\"ApacheTransform\", ApacheTransform())\n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Server_Pipeline = Pipeline([\n",
    "    (\"column_selector\", Column_selector(col_indexes=[3])),\n",
    "     #Effettuo l'UPPERCASE della colonna selezionata e la passo al transform successivo\n",
    "    (\"check_nan\", CheckNan()),\n",
    "    (\"StringUpper\", StringUpper()),\n",
    "    #Prendo l'UPPERCASE e trasformo tutto in binario\n",
    "    (\"ColumnLabelBinarizer\", ColumnLabelBinarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server_feature_union = FeatureUnion([\n",
    "    (\"server_pipeline_1\", Server_Apache_Pipeline),\n",
    "    (\"Server_Pipeline\", Server_Pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cache_Pipeline = Pipeline([\n",
    "    (\"column_selector\", Column_selector(col_indexes=[4])),\n",
    "    (\"StringUpper\", StringUpper()), \n",
    "    (\"CacheHandler\", CacheHandler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_length_pipeline = Pipeline([\n",
    "    ('column_selector', Column_selector(col_indexes = [5])),\n",
    "    ('imputer', Imputer(strategy = \"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countryPipeline = Pipeline([\n",
    "    ('column_selector', Column_selector(col_indexes=[6])),\n",
    "    ('checkNan', CheckNan()),\n",
    "    ('stringUpper', StringUpper()),\n",
    "    ('columnLabelBinarizer',ColumnLabelBinarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "provincePipeline = Pipeline([\n",
    "    ('column_selector', Column_selector(col_indexes=[7])),\n",
    "    ('checkNan', CheckNan()),\n",
    "    ('stringUpper', StringUpper()),\n",
    "    ('columnLabelBinarizer',ColumnLabelBinarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portPipeline = Pipeline([\n",
    "    ('column_selector', Column_selector(col_indexes=[10])),\n",
    "    ('checkNan', CheckNan()),\n",
    "    ('stringUpper', StringUpper()),\n",
    "    ('columnLabelBinarizer',ColumnLabelBinarizer())  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_pipeline = Pipeline([\n",
    "    (\"column_selector\", Column_selector([8])),\n",
    "    (\"extension_extractor\", extension_extractor()),\n",
    "    (\"label_binarizer\", ColumnLabelBinarizer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_and_scale1 = Pipeline([\n",
    "    (\"column_selector\", Column_selector([0, 1])),\n",
    "    (\"checkNumberNan\", NumberCheckNan()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_and_scale2 = Pipeline([\n",
    "    (\"column_selector\", Column_selector([9])),\n",
    "    (\"checkNumberNan\", NumberCheckNan()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_and_scale3 = Pipeline([\n",
    "    (\"column_selector\", Column_selector([11, 12, 13, 14, 15, 16, 17, 18, 19, 20])),\n",
    "    (\"checkNumberNan\", NumberCheckNan()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_pipeline = FeatureUnion([\n",
    "    #(\"add_url_length\", Column_selector([0])),\n",
    "    #(\"add_num_special_charater\", Column_selector([1])),\n",
    "    ('select_and_scale1', select_and_scale1),\n",
    "    (\"charset_pipeline\", charsetPipeline),\n",
    "    (\"server_pipeline\", server_feature_union),\n",
    "    (\"cache_pipeline\", Cache_Pipeline),\n",
    "    (\"content_length_pipeline\", content_length_pipeline),\n",
    "    (\"countryPipeline\", countryPipeline),\n",
    "    (\"provincePipeline\", provincePipeline),\n",
    "    (\"domain_pipeline\", domain_pipeline),\n",
    "    #(\"add_col_9\", Column_selector([9])),\n",
    "    ('select_and_scale2', select_and_scale2),\n",
    "    (\"portPipeline\", portPipeline),\n",
    "    ('select_and_scale3', select_and_scale3)\n",
    "    #(\"add_remaining_cols_\", Column_selector([11, 12, 13, 14, 15, 16, 17, 18, 19, 20]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definisco una pipeline che mette insieme la prePipeline e la full_pipeline, in modo da non dover fare due invocazioni\n",
    "\n",
    "data_preparator = Pipeline([\n",
    "    (\"prePipeline\", prePipeline),\n",
    "    (\"full_pipeline\", full_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:42: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:42: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Preparo i dati per gli algoritmi\n",
    "\n",
    "train_set_prepared = data_preparator.fit_transform(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definisco un label_binarizer, questo servirà a trasformare in binario le etichette 'Benigno'/'Maligno'\n",
    "\n",
    "label_binarizer = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1424, 645)"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Qui possiamo vedere la struttura del train_set trasformato\n",
    "\n",
    "train_set_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Otteniamo le etichette binarizzate per il train_set. Notare che qui viene invocato il metodo .fit_transform() !!\n",
    "\n",
    "train_set_labels = label_binarizer.fit_transform(strat_train_set[\"TIPO\"])\n",
    "\n",
    "# Usiamo la funzione flatten per modificare la formattazione di serie del vettore che è\n",
    "# del tipo v = [[e1], [e2], [e3], ..., [en]] in v = [e1, e2, e3, ..., en]\n",
    "\n",
    "train_set_labels = train_set_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definiamo una funzione che misura l'accuratezza delle predizioni dell'algoritmo\n",
    "\n",
    "def accuracy_rate(labels, predictions):\n",
    "    count = 0\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == predictions[i]:\n",
    "            count += 1\n",
    "    return count/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importiamo il modello SVC. Esso è un Classificatore binario (riesce a classificare tra due sole classi) basato\n",
    "# sull'algoritmo SVM = Support Vector Machines.\n",
    "\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Facciamo un test preventivo per misurare l'accuratezza dell'algoritmo, lasciando i parametri di default\n",
    "\n",
    "classifier = LinearSVC(C= 1, loss = \"hinge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eseguiamo il fit (= allenamento) dell'algoritmo sul train_set\n",
    "\n",
    "classifier.fit(train_set_prepared, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Misuriamo l'accuratezza del classificatore sul train_set, ovvero sui dati sul quale è stato allenato\n",
    "\n",
    "predictions_train = classifier.predict(train_set_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859550561797753"
      ]
     },
     "execution_count": 808,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Misuriamo l'accuratezza delle predizioni\n",
    "\n",
    "accuracy_rate(train_set_labels, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L'accuratezza sembra eccessiva, facendo pensare che l'algoritmo sia andato in overfitting. Per regolarizzare\n",
    "# le predizioni, usiamo la classe GridSearchCV, la quale divide il train_set in tante coppie di \n",
    "# mini_train_set/mini_test_set ed allena l'algoritmo con vari parametri, restituendoci quelli che\n",
    "# hanno performato meglio\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# indichiamo i valori del parametri su cui vogliamo testare l'algoritmo\n",
    "\n",
    "param_grid = [\n",
    "    {'C': [.1, .5, 1, 10, 50, 100, 500]}, \n",
    "    {\"loss\": [\"hinge\", \"squared_hinge\"]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inizializziamo il classificatore\n",
    "\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.1, 0.5, 1, 10, 50, 100, 500]}, {'loss': ['hinge', 'squared_hinge']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alleniamo l'algoritmo nel modo prestabilito. Il parametri cv indica il numero di split del train_set in\n",
    "# 'mini_train_set'/'mini_test_set', e quindi le dimensioni degli stessi.\n",
    "\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(train_set_prepared, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chiediamo semplicemente a grid_search qual è la migliore combinazione possibile di parametri: il fatto che\n",
    "# la misura dell'errore non sia specificata ci indica che è indifferente ai fini delle performances\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Facciamo una ricerca più 'raffinata' per il parametro 'C': il risultato ci indica che C=1 ci assicura\n",
    "#La maggiore accuratezza\n",
    "\n",
    "grid_search = GridSearchCV(classifier, param_grid= [{'C': [7, 8.5, 1, 11.5, 13]}], cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(train_set_prepared, train_set_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9859550561797753"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Siamo pronti quindi per misurare le performances sul test_set! Innanzitutto lo alleniamo con i parametri migliori, e\n",
    "#ne misuriamo nuovamente l'accuratezza sul train_set\n",
    "\n",
    "classifier = LinearSVC(C= 1, loss = \"hinge\")\n",
    "classifier.fit(train_set_prepared, train_set_labels)\n",
    "predizioni = classifier.predict(train_set_prepared)\n",
    "accuracy_rate(train_set_labels, predizioni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanni/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9859550561797753"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A questo punto calcoliamo le predizioni sul test_set (dati che l'algoritmo non ha mai visto), e ne misuriamo \n",
    "# l'accuratezza. \n",
    "\n",
    "# Innanzitutto prepariamo i dati e calcoliamo le etichette (notare che in questo caso vengono chiamati i metodi\n",
    "# .transform() e non fit_transform() !! )\n",
    "\n",
    "test_set_prepared = data_preparator.transform(strat_test_set)\n",
    "test_set_labels = label_binarizer.transform(strat_test_set[\"TIPO\"])\n",
    "predizioni_test = classifier.predict(test_set_prepared)\n",
    "\n",
    "accuracy_rate(test_set_labels, predizioni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Possiamo vedere che l'accuratezza sul test_set (= dati che l'algoritmo non ha mai visto) raggiunge il 98.6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alleniamo adesso un classificatore basato su reti neurali trmite tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un oggetto iterabile sulle colonne del test_set\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(train_set_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12db5bf98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': None}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0\n"
     ]
    }
   ],
   "source": [
    "# Creiamo una istanza di DNNClassifier, un classificatore basato su reti neurali profonde. In questo caso abbiamo tre\n",
    "#'livelli nascosti', che si compongono rispettivamente di 100, 70 e 50 neuroni. Abbiamo scelto di usare\n",
    "# una rete a tre livelli (ma con un numero moderato di neuroni) piuttosto che avere 1-2 livelli ma più ampi,\n",
    "# poiché aumentare la profondità della rete assicura performances migliori rispetto ad aumentarne l'ampiezza.\n",
    "\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[100, 70, 50], n_classes = 2, feature_columns = feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Questo cast è necessario per le versioni più recenti di TesorFlow\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From /Users/giovanni/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:615: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0/model.ckpt-250\n",
      "INFO:tensorflow:Saving checkpoints for 251 into /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0333497, step = 251\n",
      "INFO:tensorflow:global_step/sec: 109.906\n",
      "INFO:tensorflow:loss = 0.0433373, step = 351 (0.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 138.898\n",
      "INFO:tensorflow:loss = 0.0457218, step = 451 (0.661 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 550 into /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0116877.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eseguiamo il training dell'algoritmo. Con questi parametri l'algoritmo ripete 500 iterazioni del training e,\n",
    "# ad ogni iterazione, considera 40 elementi del train_set (scelti uniformemente a caso) \n",
    "\n",
    "dnn_clf.fit(train_set_prepared, train_set_labels, batch_size = 40, steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0/model.ckpt-550\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo le predizioni riguardo al train_set\n",
    "\n",
    "predizioni_train = dnn_clf.predict(train_set_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9985955056179775"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcoliamo l'accuratezza delle precisioni\n",
    "\n",
    "accuracy_rate(train_set_labels, predizioni_train[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/1g/gcgbjxpj23g6k44tq1_24zv80000gp/T/tmpykfjn3d0/model.ckpt-550\n"
     ]
    }
   ],
   "source": [
    "# L'accuratezza sul test_set risulta molto alta: ciò può essere dovuto sia alla eccessiva complessità del modello,\n",
    "# che può aver appreso anche il 'rumore' presente sui dati, andando in overfitting, sia al fatto che il modello\n",
    "# risulti effettivamente accurato. Pur essendo l'accuratezza così alta, essa potrebbe essere comunque indice di\n",
    "# un modello accurato siccome questo numero non si discosta tanto dalla accuratezza ottenuta tramite\n",
    "# il classificatore SVM. Perciò, ci limiteremo a controllare se il modello è affidabile o meno, semplicemente\n",
    "# controllando l'accuratezza sul test_set.\n",
    "\n",
    "predicted_test = dnn_clf.predict(test_set_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971910112359551"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rate(test_set_labels.flatten(), predicted_test[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Proviamo inoltre ad allenare un ultimo modello, detto RandomForestClassifier. Esso è basato sull'algoritmo \n",
    "# DecisionTree, e consiste nel creare n_estimators Decision Trees, ognuna con un numero massimo di nodi\n",
    "# uguale a max_leaf_nodes. In questo primo caso utilizziamo dei parametri di esempio, con lo scopo di affinarli\n",
    "# successivamente\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 100, max_leaf_nodes = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=13,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 925,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alleniamo l'algoritmo tramite il metodo .fit()\n",
    "\n",
    "rnd_clf.fit(train_set_prepared, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9367977528089888"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Misuriamo l'accuratezza delle predizioni sui dati sui quali l'algoritmo è stato allenato.\n",
    "\n",
    "predizioni_train = rnd_clf.predict(train_set_prepared)\n",
    "accuracy_rate(predizioni_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=13,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'max_leaf_nodes': [5, 8, 10, 12, 15]}, {'n_estimators': [50, 75, 100, 175, 250, 500]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 931,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizziamo la classe grid_search per fare diversi test sugli iperparametri dell'algoritmo\n",
    "\n",
    "param_grid = [\n",
    "    {\"max_leaf_nodes\": [5, 8, 10, 12, 15]},\n",
    "    {'n_estimators': [50, 75, 100, 175, 250, 500]}\n",
    "]\n",
    "grid_search = GridSearchCV(rnd_clf, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(train_set_prepared, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 15}"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chiediamo quindi a grid_search quali siano i parametri migliori: la risposta è {'max_leaf_nodes': 15}, ad indicare\n",
    "# che il parametro n_estimators non impatta in modo significativo l'accuratezza\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 19}"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eseguiamo quindi una ricerca più mirata sul parametro max_leaf_nodes\n",
    "\n",
    "param_grid = [\n",
    "    {\"max_leaf_nodes\": [13, 14, 15, 16, 17, 18, 19]},\n",
    "    {'n_estimators': [50]}\n",
    "]\n",
    "grid_search = GridSearchCV(rnd_clf, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(train_set_prepared, train_set_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 25}"
      ]
     },
     "execution_count": 946,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ancora...\n",
    "\n",
    "param_grid = [\n",
    "    {\"max_leaf_nodes\": [19, 20, 21, 22, 23, 24, 25, 26]},\n",
    "    {'n_estimators': [50]}\n",
    "]\n",
    "grid_search = GridSearchCV(rnd_clf, param_grid, cv = 5, scoring = 'neg_mean_squared_error')\n",
    "grid_search.fit(train_set_prepared, train_set_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550561797752809"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eseguiamo un training dell'algoritmo con i parametri indicati da grid_search, e ne misuriamo l'accuratezza\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 50, max_leaf_nodes = 24)\n",
    "rnd_clf.fit(train_set_prepared, train_set_labels)\n",
    "predizioni_train = rnd_clf.predict(train_set_prepared)\n",
    "accuracy_rate(predizioni_train, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466292134831461"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testiamo l'algoritmo sul test_set\n",
    "\n",
    "predizioni_test = rnd_clf.predict(test_set_prepared)\n",
    "accuracy_rate(predizioni_test, test_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Proviamo a mettere insieme tutti e tre i classificatori costruiti fino ad ora tramite la classe\n",
    "# BaggingClassifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
