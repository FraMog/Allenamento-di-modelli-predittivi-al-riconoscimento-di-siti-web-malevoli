# Allenamento di alcuni modelli predittivi al riconoscimento di siti web malevoli

- Linguaggio di Programmazione: 
  PYTON

- Librerie Utilizzate:
  NumPy,
  Pandas,
  ScikitLearn,
  TensorFlow,
  
- Ambiente di Sviluppo: 
  JUPYTER

- Outline

Il nostro lavoro si è incentrato sulla formulazione di un modello predittivo di tipo probabilistico, in grado di stimare la presenza di codice malevolo su siti Web. Si è partiti da un dataset, rappresentato da una tabella di n + 1 colonne. All’interno del dataset, ogni colonna indica uno speciﬁco attributo del sito web in questione; ci si riferisce comunemente agli attributi come feature. Le iniziali n colonne rappresentano dati di vario tipo, tra cui informazioni sulla struttura dell’URL, la nazione dove è ﬁsicamente situato il server in cui risiede il sito, ed altro. L’ultima colonna, in posizione n + 1, contiene invece una informazione booleana che identiﬁca un sito come malevolo o non malevolo. Nel dataset in questione il 12.13% dei siti è identiﬁcato come malevolo, mentre il restante 87.87% dei siti è identiﬁcato come non malevolo. L’insieme di dati preso in considerazione contiene informazioni reali su siti internet, scelti in modo da costituire un campione (ragionevolmente) casuale del Web. Questa considerazione è di fondamentale importanza per valutare l’utilità del modello realizzato: non avrebbe senso infatti tentare di costruire un modello statistico su un campione che non rappresenta la realtà. Altre considerazioni fondamentali riguardo il campione di dati sono quelle a proposito della taglia: è molto difficile rappresentare un insieme di dati estremamente grande come quello dei siti Web con un dataset piccolo; questo requisito si scontra però con quello della fattibilità: la gestione di dataset molto grandi è estremamente costosa dal punto di vista computazionale. La nostra attività è stata divisa in più fasi. Una prima fase di preprocessing iniziale ci ha permesso di eseguire una pulizia dei dati ad alto livello. Successivamente è stato necessario effettuare una seconda fase di preprocessing, con lo scopo di adattare i dati agli algoritmi da utilizzare. Questo si è reso necessario poich´e gran parte degli algoritmi di machine learning non accetta input di tipo stringa, oppure richiede che gli attributi numerici siano codiﬁcati in modi speciﬁci. In ultimo abbiamo costruito i modelli e ne abbiamo testato le performance. Riguardo alle performance, il nostro obiettivo è stato quello di superare in modo signiﬁcativo l’accuratezza di ’predizioni banali’, possibili conoscendo semplicemente la distribuzione dei dati nel dataset. Ad esempio, sapendo che l’87.87% delle istanze è classiﬁcato come malevolo, basterebbe predire ogni volta un sito come non malevolo, per ottenere un’accuratezza dell’87.87%. Per la realizzazione implementativa del progetto è stato utilizzato Python, il linguaggio più diffuso nel campo della data science; in particolar modo sono state sfruttate le librerie NumPy, Pandas, ScikitLearn e TensorFlow, le quali offrono strumenti matematici ed algoritmici per effettuare tutte le principali operazioni di machine learning, compresa la parte di preprocessing. Inﬁne, invece di sfruttare un ambiente di sviluppo integrato, abbiamo optato per l’utilizzo dell’ambiente Jupyter Notebook, anch’esso fondamentale nel campo della data science, in quanto risulta essere molto più agile rispetto ad un normale IDE, permettendo di compilare ed eseguire istruzioni singolarmente. Riguardo la scelta del dataset, sul Web vi sono numerosi siti che offrono risorse di questo tipo, e numerosissimi sono i dataset stessi, riguardanti gli argomenti più disparati, da poter sfruttare come base per elaborazioni di tipo machine learning. Tra le varie tematiche, tuttavia, la sicurezza informatica presenta un numero di dataset più esiguo rispetto a quelli disponibili relativamente ad altri campi. Ciò potrebbe essere dovuto alla natura intrinseca della disciplina. Inoltre, solitamente, i dataset reperibili in rete relativi alla sicurezza informatica, presentano dati ’semplici’ (ovvero dotati di poche features), e spesso già pronti (ovvero dati che possono essere direttamente forniti agli algoritmi di machine learning, senza bisogno di uno step intermedio di preparazione). Uno dei siti web più famosi da cui è possibile attingere per ottenere dataset è Kaggle, dal quale è stato prelevato il dataset di cui ci occuperemo. La scelta del dataset è stata effettuata in base a vari fattori, tra i quali il numero di features (24 + 1, un valore sufficientemente elevato da non richiedere un preprocessing ’banale’), e la presenza di numerose colonne con dati grezzi, come stringhe e valori nulli. Questo aspetto è stato considerato poich´e la maggioranza degli algoritmi di Machine Learning opera esclusivamente su dati numerici, che a volte devono essere addirittura codiﬁcati in modo speciﬁco. Ciò ha reso necessaria una laboriosa fase di preprocessing, che ci ha permesso di prendere conﬁdenza con le principali tecniche adoperate in questo settore. Un altro fattore di cui abbiamo tenuto conto, è il fatto che il dataset in questione è molto recente: i dati sono stati collezionati e pubblicati negli ultimi mesi. Oltre a dare un senso ad eventuali applicazioni pratiche di modelli realizzati tramite questi dati, ciò garantisce che essi siano stati collezionati con la consapevolezza delle possibilità offerte oggi dal machine learning. L’implementazione del progetto è stata suddivisa in diverse fasi: inizialmente sono stati esplorati brevemente i dati, per comprenderne la strutturazione. Successivamente, per ogni colonna che lo richiedesse, è stata effettuata la fase di preprocessing (ovvero di preparazione dei dati agli algoritmi). In ultimo è stata effettuata la fase di analisi statistico-probabilistica. Relativamente a quest’ultima fase, inizialmente sono stati allenati due algoritmi di machine learning, uno basato su SupportVector Machines, ed uno basato su Random Forest. Successivamente è stato allenato un classiﬁcatore basato su DeepNeuralNetworks. In tutti e tre i casi, per l’allenamento e l’impostazione degli iperparametri degli algoritmi ci siamo serviti degli strumenti forniti da ScikitLearn, i quali sono in grado di fornire indicazioni precise circa le performance degli algoritmi relativamente a diverse conﬁgurazioni di iperparametri, testando ripetutamente gli algoritmi con settaggi differenti. A questo punto gli algoritmi sono stati allenati con gli iperparametri ritenuti migliori e ne sono state valutate le performance. Dopo aver costruito i modelli con i parametri ritenuti migliori, ed averne analizzate le performance, si è notato che sui dati di test, ovvero dati sconosciuti agli algoritmi, tramite i modelli basati su SupportVectorMachines e DeepNeuralNetwork si raggiunge un’accuratezza maggiore del 97%. Nel caso del modello basato su RandomForest, sempre sui dati di test, l’accuratezza si aggira intorno al 95%. In tutti e tre i casi si è comunque raggiunto l’obiettivo preﬁssato si superare in modo signiﬁcativo l’accuratezza della ”previsione banale”, ovvero l’87.87%.
